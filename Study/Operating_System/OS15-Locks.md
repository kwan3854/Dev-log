# Locks

여러 개의 명령어들을 원자적으로 실행해보고 싶지만, 단일 프로세서의 인터럽트로 인해서 (또는 멀티 쓰레드를 여러 프로세서에 병행성하려고 해서) 그렇게 할 수가 없었다.

**락(lock)** 을 이용해서 이 문제를 직접적으로 다루고자 한다.

프로그래머들은 소스 코드의 임계 영역을 락으로 둘러서 그 임계 영역이 마치 하나의 원자 단위 명령어인것처럼 실행되도록 한다.

## 1. 락: 기본 개념

예를들어 다음의 임계 영역 (critical section) 이 있다고 해 보자.

```c
balance = balance + 1;
```

락을 사용하기 위해서 락으로 임계 영역을 다음과 같이 감쌌다.

```c
lock_t mutex;
...
lock(&mutex);
balance = balance + 1;
unlock(&mutex);
```

락은 일종의 변수이다.

락을 사용하기 위해서는 락 변수를 먼저 선언해야 한다.

이 **락 변수** 는 락의 상태를 나타낸다.

락은 둘 중 하나의 상태를 갖는다.

1. **사용 가능(available) 상태 (unlocked 또는 free)**

   어떤 쓰레드도 락을 소유하고 있지 않은 상태

2. **사용 중 (acquired) 상태**

   임계 영역에서 정확히 하나의 쓰레드가 락을 획득한 상태.

lock() 과 unlock() 루틴의 의미는 간단하다.

lock() 루틴 호출을 통해 락 획득을 시도한다.

만약 어떤 쓰레드도 갖고 있지 않으면 그 쓰레드는 락을 획득하여 임계 영역 내로 진입한다.

이렇게 락을 획득한 쓰레드를 락 **소유자(owner)** 라고 부른다.

만약 다른 쓰레드가 lock() 을 호출할 경우, 락이 사용중인 동안에서는 lock() 함수는 리턴하지 않는다.

락을 소유한 쓰레드가 임계 영역에 존재하는 상태에서는 다른 쓰레드들이 임계 역역으로 진입할 수가 없다.

락을 통해 프로세스들의 혼란스런 실행 순서에 어느 정도의 질서를 부여할 수 있다.

---

### 락 구현시 필요요건

- **Correctness**
  - **Mutual exclusion:** 한번에 하나의 쓰레드만 critical section 에 들어갈 수 있다.
  - **Progress (deadlock-free):** 만약 여러 쓰레드가 critical section 에 들어가고 싶어하면 반드시 하나는 들어가게 해줘야 한다.
  - **Bounded waiting (starvation-free):** 각각의 대기중인 쓰레드는 결국에는 들어가게 해 줘야 한다.
- **Fairness**
  - 각각의 쓰레드는 lock 을 얻는데 fair 한 기회를 가진다.
- **Performance**
  - 경쟁이 있을 때와 없을 때의 락의 time overhead 를 고려해야 한다.

## 2. Pthread 락

쓰레드 간에 **상호 배제(mutual exclusion)** 기능을 제공하기 때문에 POSIX 라이브러리는 락을 **mutex** 라고 부른다.

상호 배제는 한 쓰레드가 임계 영역 내에 있다면 이 쓰레드의 동작이 끝날 때까지 다른 쓰레드가 임계 영역에 들어 올 수 없도록 제한한다고 해서 얻은 이름이다.

```c
Pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
Pthread_mutex_lock(&lock); // 래퍼. 실패시 exit
balance = balance + 1;
Pthread_mutex_unlock(&lock);
```

하나의 락으로 모든 임계 영역들을 보호하는 것이 아니라(coarse-grained 락 사용 전략), 각 데이터와 자료 구조를 보호하는 데 있어서, 여러 락을 사용한다.

다수의 쓰레드가 서로 다른 락으로 보호된 코드를 실행할 수 있다(fine-grained 락 사용전략).

## 3. 락의 구현

> **핵심 질문: 락은 어떻게 만들까**
>
> 효율적인 락은 어떻게 만들어야 하는가? 효율적인 락은 낮은 비용으로 상호 배제 기법을 제공하고 몇 가지 속성들을 추가로 가져야 한다. 어떤 하드웨어 지원이 필요한가? 어떤 운영체제 지원이 필요한가?

오랜 기간을 걸쳐 다양한 컴퓨터 구조의 명령어 집합에 여러 하드웨어 명령어들이 추가되었다.

정교한 락 라이브러리를 제작하는 데 있어 운영체제가 관여하는 것은 무엇인지 알아보자.

## 4. 락의 평가

락 설계시, 락의 정상동작 여부 판단을 위한 평가기준을 정해야 한다.

1. **상호 배제** 를 제대로 지원하는가

   락이 동작하여 임계 영역 내로 다수의 쓰레드가 진입을 막을 수 있는지 검사.

2. **공정성(fairness)**

   쓰레드들이 락 획득에 대한 공정한 기회가 주어지는가?

   락을 전혀 얻지 못해 **굶주리는(starve)** 경우가 발생하는가?

3. **성능(performance)**

   락 사용의 시간적 오버헤드를 평가.

   1. 경쟁이 전혀 없는 경우의 성능 (하나의 쓰레드가 실행 중에 락을 획득하고 해제할때의 부하)
   2. 여러 쓰레드가 단일 CPU 상에서 락을 획득하고 해제하는 부하
   3. 멀티 CPU 상황에서 락 경쟁 시의 성능

## 5. 인터럽트 제어

초기의 단일 프로세스 시스템에서는 상호 배제 지원을 위해 임계 영역 내에서 인터럽트를 비활성하는 방법을 사용했었다.

```c
void lock() {
  DisableInterrupts();
}
void unlock() {
  EnalbeInterrupts();
}
```

임계 영역 진입 전에 인터럽트를 비활성화 시키면, 임계 영역 내의 코드에서는 인터럽트가 발생할 수 없다.

이 방법의 **장점** 은 단순하다는 것이다. (깊이 생각하지 않아도 된다.)

이 방법은 **단점** 이 많다.

1. 이 요청을 하는 쓰레드가 인터럽트를 활성/비활성화 하는 **특권(privileged)** 연산을 실행할 수 있도록 허가해야 한다.

   이를 다른 목적으로 사용하지 않음을 신뢰할 수 있어야 한다. ~~옴닉을 믿다니 여기 사람들은 바보입니다~~

2. 멀티프로세서에서는 적용을 할 수가 없다.

   여러 쓰레드가 여러 CPU에서 실행 중이라면 각 쓰레드가 동일한 임계 영역을 진입하려고 시도할 수 있다.

   이때 어떤 프로세서의 인터럽트를 비활성화 해도 다른 프로세서에는 영향이 가지 않는다. (임계영역에 진입이 가능하다는 뜻)

3. 장시간 동안 인터럽트를 중지시키는 것은 중요한 인터럽트의 시점을 놓칠 수 있다

   때로는 시스템에 심각한 문제를 가져 올 수 있다.

   예를들어 CPU가 저장 장치에서 읽기 요청을 마친 사실을 모르고 지나갔다고 해 보자. 운영체제가 이 읽기 결과를 기다리는 프로세스를 언제 깨울 수 있을까?

4. 비효율적이다. (비교적 덜 중요)

   최신의 CPU들에서는 느리게 실행되는 경향이 있다.

위의 이유로 상호 배제를 위하여 인터럽트를 비활성화하는 것은 제한된 범위에서만 사용되어야 한다.

예컨데, 운영체제가 내부 자료구조에 대한 원자적 연산을 위해 인터럽트를 비활성화할 수 있다.

운영체제 내부에서는 신뢰라는 문제가 사라지기 때문에 운영체제가 특혜 동작을 어떤 방식으로 처리하든 인터럽트를 비활성화하더라도 용인할 수 있다.

## 6. 실패한 시도: 오직 Load/store 명령어만 사용하기

인터럽트 활성화/비활성화 방법을 사용하지 않고 락을 구현하려면 CPU 하드웨어와 락 구현을 위한 명령어를 사용해야만 한다.

먼저 단일 플래그 변수를 사용하여 간단한 락을 구현해 보자.

(하나의 변수와 일반적인 load와 store 명령어만으로는 락의 구현이 불가능함을 보여준다.)

```c
typedef struct __lock_t { int flag; } lock_t;

void init(lock_t *mutex) {
  // 0 -> 락이 사용 가능함, 1 -> 락 사용 중
  mutex->flag = 0;
}

void lock(lock_t *mutex) {
  while (mutex->flag == 1) // flag 변수를 검사(TEST) 함
    ; // spin-wait (do nothing)
  mutex->flag = 1; // 이제 설정(SET) 한다!
}

void unlock(lock_t *mutex) {
  mutex->flag = 0;
}
```

간단한 변수(flag)를 사용하여 쓰레드가 락을 획득하였는지를 나타낸다.

이 코드에는 두 가지 문제가 있다.

1. 제대로 동작하는가 여부

   적시에 인터럽트가 발생하면 두 쓰레드 모두 플래그를 1로 설정하는 경우가 생길 수 있다.

2. 성능

   spin-wait 라는 방법을 사용하여 플래그의 값을 무한히 검사하는데, 이 방법은 다른 쓰레드가 락을 해제할 때까지 시간을 낭비한다.

*이 방법의 핵심 문제점은 critical section의 동시성 문제를 해결하기 위해 만든 것이, 또 다른 critical section 이 된다는 것이다.*

## 7. Test-And-Set 을 사용하여 작동하는 스핀 락 구현하기

위 방법들의 문제 떄문에, 시스템 설계자들은 락 지원을 위한 하드웨어를 설계하기 시작했다.

하드웨어 기법 중 가장 기본은 **test-and-set** 명령어 또는 **원자적 교체(atomic exchange)** 로 알려진 명령어이다.

우리는 test-and-set 의 동작을 다음과 같은 C 코드로 정의한다. (하드웨어의 도움을 받아 실행됨)

```c
int TestAndSet(int *old_ptr, int new) {
  int old = *old_ptr; // old_ptr 의 이전 값을 가져옴
  *old_ptr = new; // old_ptr 에 'new' 의 값을 저장함
  return old; // old 의 값을 반환함
}
```

ptr이 가리키고 있던 예전의 값을 반환하고 동시에 new에 새로운 값을 저장한다.

여기서 핵심은 이 동작들이 원자적으로 수행된다는 것이다.

"test and set" 이라고 부르는 이유가 이전 값을 "검사(test)" 하는 동시에 메모리에 새로운 값을 "설정(set)" 하기 때문이다.

이 명령어만으로 간단한 스핀 락을 만들 수 있다.

```c
typedef struct __lock_t {
  int flag;
} lock_t;

void init(lock_t *lock) {
  // 0 은 락이 획득 가능한 상태를 표시, 1 은 락을 획득했음을 표시
  lock->flag = 0;
}

void lock(lock_t *lock) {
  while (TestAndSet(&lock->flag, 1) == 1)
    ; // 스핀(아무 일도 하지 않음)
}

void unlock(lock_t *lock) {
  lock->flag = 0;
}
```

락의 값을 검사(test) 하고 새로운 값으로 설정(set) 하는 동작을 원자적 연산으로 만듦으로써 오직 하나의 쓰레드만 락을 획득할 수 있도록 만들었다.

이 **스핀 락** 이 가장 기초적인 형태의 락으로서, 락을 획득할 때까지, CPU 사이클을 소모하면서 회전한다.

단일 프로세서에서 이 방식을 제대로 사용하려면 **선점형 스케줄러(preemptive scheduler)** 를 사용해야 한다.

## 8. 스핀 락 평가

- 정확성

  제대로 임의의 시간에 단 하나의 쓰레드만이 critical section에 진입하게 하는 제대로 동작하는 락이다.

- 공정성

  대기 중인 쓰레드에게 임계 영역에 진입할 기회가 주어지는 것을 보장하는가?

  스핀 락은 어떤 공정성도 보장하지 않는다.

- 성능

  1. 단일 프로세서의 경우

     성능 오버헤드가 상당히 클 수 있다.

  2. 여러 CPU에 쓰레드가 퍼져 있는 경우

     쓰레드의 개수가 CPU의 개수와 대충 같다면 스핀락은 꽤 합리적으로 동작한다.

## 9. Compare-And-Swap (가장 중요하다)

또 다른 하드웨어 기법은 SPARC의 **Compare-And-Swap** , x86에서는 **Compare-And-Exchange** 가 있다.

```c
int CompareAndSwap(int *ptr, int expected, int new) {
  int original = *ptr;
  if (original == expected)
    *ptr = new;
  return original;
}
```

Compare and swap 기법의 기본 개념은 ptr가 가리키고 있는 주소의 값이 expected 변수와 일치하는지 검사하는 것이다.

만약 일치한다면 ptr이 가리키는 주소의 값을 새로운 값으로 변경한다. 불일치한다면 아무 것도 하지 않는다.

다음과 같은 방식으로 락을 만들 수 있다.

```c
void lock(lock_t *lock) {
  while (CompareAndSwap(&lock->flag, 0, 1) == 1)
}
```

이 이후의 코드는 앞의 Test-And-Set 의 코드와 동일하다.

CompareAndSwap 명령어는 TestAndSet 명령어보다 더 강력하다.

단순히 스핀 락과 같은 식으로 사용하면 스핀락과 다를바 없지만,

**대기 없는 동기화 (wait-free synchronization)** 와 같은 주제를 다룰 때 이 루틴의 능력을 알게 될 것이다.

## 10. Load-Linked 그리고 Store-Conditional

MIPS 구조에서는 **load-linked** 와 **store-conditional** 명령어를 앞뒤로 사용하여 락이나 기타 병행 연산을 위한 자료구조를 만들 수 있다.

이 명령어들에 대한 C 의사 코드는 다음과 같다. (Alpha, PowerPC, ARM 에서도 유사한 명령어를 지원한다)

```c
int LoadLinked(int *ptr) {
  return *ptr;
}

int StoreConditional(int *ptr, int value) {
  if (no updat to *ptr since the LoadLinked to this address) {
    *ptr = value;
    return 1; // 성공!
  } else {
    return 0; // 갱신 실패
  }
}
```

load-linked 는 일반 load 명령어처럼 메모리 값을 레지스터에 저장한다.

실제 차이는 store-conditional 명령어에서 나타난다.

store-conditional 명령어는 동일한 주소에 다른 store 가 없었던 경우에만 저장을 성공한다.

저장이 성공하면, load-linked 가 탑재했던 값을 갱신한다.

성공한 경우에는 store-conditional 은 1을 반환하고 ptr이 가리키는 value의 값을 갱신한다.

실패한 경우에는 ptr이 가리키는 value의 값이 갱신되지 않고 0을 반환한다.

```c
void lock(lock_t *lock) {
  while (1) {
    while (LoadLinked(&lock->flag) == 1)
      ; // 0이 될 때까지 스핀
    if (StoreConditional(&lock->flag, 1) == 1)
      return; // 1로 변경하는 것이 성공하였다면: 완료
              // 아니라면: 처음부터 다시 시도
  }
}

void unlock(lock_t *lock) {
  lock->flag = 0;
}
```

lock을 다음과 같이 줄여서 표현할 수도 있다 똑같이 동작한다.

```c
void lock(lock_t *lock) {
  while (LoadLinked(&lock->flag) ||
        !StoreConditional(&lock->flag, 1))
    ; // 회전
}
```

## 11. Fetch-And-Add

마지막 하드웨어 기반의 기법은 **Fetch-And-Add** 명령어로 원자적으로 특정 주소의 예전 값을 반환하면서 값을 증가시킨다.

```c
int FetchAndAdd(int *ptr) {
  int old = *ptr;
  *ptr = old + 1;
  return old;
}
```

이를 이용하여 **티켓 락** 이라는 것을 만들어보자.

하나의 변수만을 사용하는 대신 이 방법에서는 티켓(ticket) 과 차례(turn) 조합을 사용하여 락을 만든다.

```c
typedef struct __lock_t {
  int ticket;
  int turn;
} lock_t;

void lock_init(lock_t *lock) {
  lock->ticket = 0;
  lock->turn = 0;
}

void lock(lock_t *lock) {
  int myturn = FetchAndAdd(&lock->ticket);
  while (lock->turn != myturn)
    ; // 회전
}

void unlock(lock_t *lock) {
  FetchAndAdd(&lock->turn);
}
```

## 12. 요약: 과도한 스핀

지금까지 소개한 하드웨어 기반의 락은 간단하고, 제대로 동작한다.

하지만 때로는 이러한 해법이 효율적이지 않은 경우도 있다.

> **핵심 질문: 회전을 피하는 방법**
>
> 어떻게 하면 스핀에 CPU 시간을 낭비하지 않는 락을 만들 수 있을까?

하드웨어의 지원으로만 이 문제를 해결할 수 없다.

운영체제로부터의 지원이 추가로 필요하다.

## 13. 간단한 접근법: 조건 없는 양보

문맥 교환이 되어 쓰레드가 실행이 되었지만 이전 쓰레드가 인터럽트에 걸리기 전에 락을 이미 획득한 상태라서 그 쓰레드가 락을 해제하기를 기다리며 스핀만 무한히 하는 경우에 어떻게 해야 할 것인가?

첫 번째 방법은 단순하고 친근한 방법이다.

락이 해제되기를 기다리며 스핀해야 하는 경우 자신에게 할당된 CPU를 다른 쓰레드에게 양보하는 것이다.

```c
void init() {
  flag = 0;
}

void lock() {
  while (TestAndSet (&flag, 1) == 1)
    yield(); // CPU를 양보함
}

void unlock() {
  flag = 0;
}
```

yield 동작은 스케줄 대상에서 자신을 빼는 것이나 마찬가지이다.

단일 CPU 시스템에서 두 개의 쓰레드를 실행하는 경우, 잘 동작한다.

100정도의 쓰레드들이 락을 획득하기 위해 경쟁하는 경우를 살펴보자.

99개의 쓰레드가 실행되고 양보하는 패턴으로 동작하게 된다.

99개의 시간 간격을 낭비하는 회전 방식보다는 낫지만, 아직 비용이 만만치 않다.

더 안 좋은 것은 굶주림 문제는 전혀 해결하지 못한다는 것이다. (어떤 쓰레드는 영원히 양보만 할 수도 있다)

## 14. 큐의 사용: 스핀 대신 잠자기

이전 방법들의 근본적인 문제는 운에 너무 많은 부분을 맡긴다는 것이다.

다수의 쓰레드가 락을 대기하고 있을 경우, 다음으로 락을 획득할 쓰레드를 명시적으로 선택할 수 있어야 한다.

이를 위해서는 운영체제의 적절한 지원과 큐를 이용한 대기 쓰레드들의 관리가 필요하다.

**두 개의 호출문이 있다.**

- **park()**

  호출하는 쓰레드를 잠재우는 함수

- **unpark(threadID)**

  threadID로 명시된 특정 쓰레드를 깨우는 함수이다.

이미 사용중인 락을 요청하는 프로세스를 재우고 해당 락이 해제되면 깨우도록 하는 락을 제작하는데 앞뒤로 사용할 수 있다.

1. 앞서 살펴 본 Test-And-Set 개념을 락 대기자 전용 큐와 함께 사용하여 더 효율적인 락을 만든다.
2. 큐를 사용하여 다음으로 락을 획득할 대상을 제어하여 기아 현상을 피할 수 있도록 한다.

```c
typedef struct __lock_t {
  int flag;
  int guard;
  queue_t *q;
} lock_t;

void lock_init(lock_t *m) {
  m->flag = 0;
  m->guard = 0;
  queue_init(m->q);
}

void lock(lock_t *m) {
  while (TestAndSet(&m->guard, 1) == 1)
    ; // 회전하면서 guard 락을 획득
  if (m->flag == 0) {
    m->flag = 1; // 락을 획득함
    m->guard = 0;
  } else { // 이부분이 원자적으로 실행되지 않으면 문제 발생 가능
    queue_add(m->q, gettid()); 
    m->guard = 0;
    // 이 순간에 가드는 풀렸다. 이때 다른 쓰레드가 unlock 하면 문제 발생
    park();
  }
}

void unlock(lock_t *m) {
  while (TestAndSet(&m->guard, 1) == 1)
    ; // 회전하면서 guard 락을 획득
  if (queue_empty(m->q))
    m->flag = 0; // 락을 포기함: 누구도 락을 원치 않음
  else
    unpark(queue_remove(m->q)); // 락을 유지함 (다음 쓰레드를 위해!)
  m->guard = 0;
}
```

guard 변수를 이용한 스핀락으로 큐와 flag 변수를 보호하고 있다.

이 방법 역시 스핀-대기를 사용하지만, 오버헤드는 상당히 작다.

쓰레드는 guard변수를 획득하거나 해제하는 과정에서 인터럽트 될 수 있다.

중요한 사실은 이 스핀락은 락과 언락 코드 내의 몇개의 명령어만 수행하면 풀린다는 것이다. 스핀 대기 시간은 상당히 짧다.

lock() 에서 쓰레드가 락을 획득할 수 없을 때 (다른 쓰레드가 가지고 있음), gettid() 함수를 호출하여 자신의 쓰레드 ID를 얻어 큐에 자신을 추가하고, guard를 0으로 설정하고 CPU를 양보한다.

쓰레드가 깨어났을 때 flag 값이 0으로 리셋되지 않았을 수도 있다.

쓰레드가 꺠어나는 시점은 park()에서 리턴된 시점이다. 이 시점에 쓰레드는 guard를 보유하지 않은 상태이다.

때문에 flag를 1로 설정할 수 조차 없다.

따라서 락을 해제하는 쓰레드는 락을 다음 쓰레드에게 직접 넘겨준다.

**이 코드는 경쟁조건이 발생할 수 있다.**

쓰레드가 park()를 호출하기 직전에 락을 소유한 쓰레드가 락을 해제하는 경우이다.

park() 를 호출한 쓰레드는 블럭상태가 된다.

이 쓰레드는 깨어날 방법이 없다. 자신을 깨워줄 쓰레드가 없기 때문이다.

> 1. 쓰레드1이 lock 의 else 문을 실행하다가 m->guard = 0; 을 하자마자 쓰레드 2가 끼어든다.
> 2. 쓰레드2는 unlock 을 실행하는데, queue가 비어있지 않기 때문에 (쓰레드1이 큐에 있음) else 문이 실행된다.
> 3. 쓰레드2는 쓰레드1을 unpark해서 깨워준다. (m->guard = 0까지 실행 될수도 안될수도 있음 상관없음.)
> 4. 쓰레드1은 다음 명령어인 park()를 실행해 잠들어버린다. (큐에 자신이 들어가 있다고 믿고. 사실은 이제 없다)
> 5. 큐에 쓰레드1이 없으므로 아무도 쓰레드1을 깨워주지 않는다. 영원히 잠들어 버린다.

이러한 문제를 **wakeup/waiting race** 라고 부른다.

Solaris 는 이 문제를 setpark() 를 추가하여 해결했다.

쓰레드가 현재 park() 를 호출하기 직전이라는 것을 표시한다.

만약 이 때 인터럽트가 실행되어 park()가 호출되기 전에 다른 쓰레드가 unpark() 를 먼저 호출한다면, 추후 park() 호출 시, 이를 호출한 쓰레드는 블럭되는 대신 바로 리턴된다.

```c
queue_add(m->q, gettid());
setpark(); // 새로운 코드
m->guard = 0;
park();
```

커널이 guard 변수의 역할을 할 수도 있다.

락의 해제와 실행 중인 쓰레드를 큐에서 제거하는 동작을 원자적으로 처리하는 것을 커널이 담당한다.

## 15. 다른 운영체제, 다른 지원 (futex)

Linux의 경우 **futex** 라는 것을 지원한다.

이것은 Solaris의 인터페이스와 유사하지만 커널 내부와 좀더 밀착되어 있다.

futex는 특정 물리 메모리 주소 그리고 커널에 정의된 큐를 가지고 있다.

호출자는 futex 관련 함수를 호출하여 잠을 자거나 잠에서 깨어난다.

- **futex_wait(address, expected)**

  address의 값과 expected의 값이 동일한 경우 쓰레드를 블럭시킨다.

  같지 않다면 리턴한다.

- **futex_wake(address)**

  큐에서 대기하고 있는 쓰레드 하나를 깨운다.

아래의 코드는 npt 1 라이브러리의 lowlevellock.h(gnu libc 라이브러리의 일부)에서 발췌한 코드이다.

이 코드는 다음과 같은 점에서 흥미롭다.

1. 이 코드에서는 하나의 정수를 이용하여 락의 사용 중 여부와(최상위 비트를 사용), 대기자 수를 동시에 표현한다(나머지 비트를 사용).

   따라서 락이 음수라면 락이 사용 중 (최상위 비트는 정수의 부호를 나타내기 때문) 인 것을 나타낸다.

2. 이 코드는 경쟁이 없는 경우에 대해 최적화 되어있다.

   하나의 쓰레드가 락 획득과 해제를 반복할 경우, 특별히 빠르게 동작한다.

```c
void mutex_lock (int *mutex) {
  int v;
  
  /* 31 번째 비트가 이미 초기화되어 있다. mutex를 이미 획득했다. 바로 리턴한다.
     (빠르게 실행하는 방법) */
  // 최상위 비트가 0이라면 test and set 에 성공해서 0이 리턴되고, 조건문이 true가 되어서 if 문 안으로 들어감.
  // lock이 안되어 있었는데 mutex_lock을 호출했으니 lock을 새로 하고 리턴한다.
  if (atomic_bit_test_set (mutex, 31) == 0) 
    return;
  atomic_increment(mutex); // 1씩 증가 시키는 함수. 2^31 개의 프로세스를 커버 가능(기다리는 프로세스의 갯수)
  while(1) {
    if (atomic_bit_test_set (mutex, 31) == 0) { // 자기 전에 한 번더 확인 (한번 자러가면 오버헤드 크기 때문)
      atomic_decrement(mutex); // 내가 증가 시켰던 1을 다시 감소 시키고 리턴
      return;
    }
    /* 이제 대기해야 한다. 먼저, 우리가 관찰 중인 futex 값이
       실제로 음수 인지 확인해야 한다(잠겨있는 상태인지). */
    v = *mutex; // 핵심 부분 (위의 if 문과 v 값 할당 부분사이에서 mutex 값이 바뀌었을 수 있다. 즉 이 mutex는 최상위비트0 일 수 있음)
    if(v >= 0) // 다시 확인. 양수라는 것은 최상위 비트가 0이라는 뜻 이는 릴리즈가 되었다는 뜻
      continue; // 그 사이에 또 mutex값이 바뀌었을 수 있기 때문에 다시 while 문 반복하면서 확인
    // 즉, v 가 최상위 비트 1인 경우(음수)에만 아래의 futex_wait() 으로 간다.
    futex_wait(mutex, v); // mutex 값에 변화가 생기면 리턴됨.
    //여기서 v는 항상 최상위비트가 1인경우(음수)이므로, mutex의 최상위 비트가 0인경우에만 futex_wait가 리턴된다. 
  }
}

void mutex_unlock (int *mutex) {
  /* 필요충분 조건으로 관심 대상의 다른 쓰레드가 없는 경우에 한해서
     0x80000000 를 카운터에 더하면 0을 얻는다 */
  if (atomic_add_zero (mutex, 0x80000000)) // 최상위비트에 1을 더하는 것과 동일. 오버플로우로 최상위 비트 0이 됨.
    return; // 만약 최상위 비트 빼고 나머지가 다 0이었다면 아무도 기다리는 쓰레드가 없다는 뜻. 그냥 리턴.
  
  /* 이 mutex를 기다리는 다른 쓰레드가 있다면
     그 쓰레드들을 깨운다. */
  futex_wake (mutex);
}
```

## 16. 2단계 락

**2단계 락(two-phase lock)** 은 락이 곧 해제될 것 같으면, 회전 대기가 더 유용하다는 점에 착안하였다.

1. 처음에는 회전하며 대기한다. 락이 빠른 시간 내에 해제될 것이라 기대하고 회전 대기한다.

   만약 첫 단계에서 락을 획득하지 못했다면 두 번째 단계로 진입한다.

2. 이 단계에서 호출자는 차단된다.

   락 해제시 블럭된 쓰레드중 하나를 잠에서 깨운다.

앞서다룬 linux 락은 이러한 형태를 가진다.

일반화된 방법은 futex가 일정 시간 동안 반복문 내에서 회전한 후에 블럭하는 것이다.

2단계 락은 하이브리드 방식이다.